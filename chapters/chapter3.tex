\chapter{Dynamical Systems}
\label{chap:ch3}

\section{Defining Ordinary Differential Equations}
\begin{definition}
  \textit{An \textbf{ordinary differential equation} is an equation of an unknown function of \textbf{one} variable. This can be expressed as a function \textbf{of} this unknown function, its corresponding variable and its various derivatives}.
\end{definition}
Its general form looks something like:
\begin{equation}\label{ODE}
  F(t,y(t),y'(t),....,y^{(n)}(t))=0,
\end{equation}
Where $y(t)$ is the unknown function of independent variable $t$ and $F : \Omega \rightarrow \mathbb{R},  \\ \Omega \subseteq \mathbb{R}^{n+1}$.

This would be what's called the implicit form of the differential equation.

\begin{definition}
  \textit{The \textbf{order} of an ODE is the highest order of the derivative present in the equation.}
\end{definition}
In our case, the order is $n$.
If, however $F$ satisfies the regularity condition \textbf{of the implicit function theorem} then the equation can be written in a much more digestible form.

The \textbf{implicit function theorem} allows one to convert a relation (implicit form) to functions of some variables. These functions may not be unique, but together, their graph may locally satisfy the relation.

As an example: The relation for a cardioid cannot be expressed as a sole function, we'd instead need the union of the graph of two separate functions for expressing it.
(si aici gen faci tu niste graphuri cu maple sau alte d-astea vezi cum faci virtual machineu ala sau mergi la faculta la un pc cu maple si iti faci lista cu ce vrei sa graphuiesti)

\textbf{Reminder}: A function $f(x)$ is continuously differentiable if $\exists f'(x)$ and $f'(x) \in C^n$ where $n>=0$

\begin{theorem}
  Let $F:D \subseteq \mathbb{R}^{n+1}\rightarrow\mathbb{R}$ be a continuously differentiable function with the relation that $F(t,y(t),y'(t),....,y^{(n)}(t))=0$. Let us express a point in the set $\mathbb{R}^{n+1} =\mathbb{R}\bigtimes\mathbb{R}^n$ as (t, \textbf{Y}) = $(t, y,y', \dots, y^{ (n) })$ and fix one such point s.t. $F(t, \textbf{Y})=0$.
  So $\exists \square_{(t, \textbf{Y})} \ni U \bigtimes V \subseteq D$ s.t. $F \in C^1(U\bigtimes V)$ and $\frac{\partial F}{\partial y}(t, \textbf{Y}) \neq 0.$ Then this means $ \exists \square_{t} \ni U_0 \subseteq U$,
  $\square_{\textbf{Y}} \ni V_0 \subseteq V$ and a function $f : U_0 \rightarrow V_0$ s.t. $f(t) = \textbf{Y}$ and $F(t,f(t))=0, \forall t \in U_0; f \in C^1(U_0)$ and $\frac{\partial f}{\partial t_i} = - \frac{\frac{\partial F}{\partial t_i}(t,f(t))}{\frac{\partial F}{\partial y}(t,f(t))}, \forall i = \overline{1,n} , \forall t \in U_0$ and $F \in C^1(U \bigtimes V),K \in N^* \Rightarrow f \in C^K(U_0).$
\end{theorem}
//TODO : proof: trust me bro

So restricting the domain $\Omega$ to one that allows the implicit form to be represented as a function of the form
\begin{equation}\label{implicit_func_theorem}
  y^{(n)}(t)=f(t,y(t),y'(t),...,y^{(n-1)}(t))
\end{equation}
would yield what's called the \textbf{explicit} form of the ODE.

A \textbf{solution} is an expression of the unknown function $y(t)$ which satisfies the relation
$y^{(n)}(t) = f(t,y(t),y'(t),\dots,y^{(n-1)}(t))$. A \textbf{general solution} includes all of these functions and usually has some constants of integration in the expression, while a \textbf{particular solution} doesn't have them.

A particular solution is usually found if we also have some initial conditions given for the unknown function, like $y(a)=b, y'(a)=c$ for some $a,b,c \in \mathbb{R}$.
Or, more formally:

\begin{definition}
  A function $y:I \rightarrow \mathbb{R}$ is a solution of the equation \ref{ODE} if the following conditions are met:

  1. $I \subseteq \mathbb{R}$ is nondegeneate interval. ($|I|>1$)

  2. $y \in C^n(I)$ and $(t,y(t), y'(t), \dots, y^{(n)}(t)) \in D_f, \forall t \in I$.

  3. $y^{(n)}(t)= f(t,y(t),y'(t),\dots,y^{ (n-1) }(t)), \forall t \in I$. ( \ref{implicit_func_theorem} is satisfied)

\end{definition}

\section{Forming Ordinary Differential Equation systems}
We may take a sequence of such first order ordinary differential equations to form a system:
\begin{equation}\label{3.2.1}
  \begin{cases}
    y'_1(t) = f_1(t,y_1(t),y_2(t),\dots,y_n(t)) \\
    y'_2(t) = f_2(t,y_1(t),y_2(t),\dots,y_n(t)) \\
    \vdots                                      \\
    y'_n(t) = f_n(t,y_1(t),y_2(t),\dots,y_n(t))
  \end{cases}
\end{equation}

Constructing $y: D \subseteq \mathbb{R} \rightarrow \mathbb{R}^n, f : D_f \subseteq \mathbb{R}^{n+1} \rightarrow \mathbb{R}^n$ and denoting \\
$y(t)= (y_1(t), \dots, y_n(t))$ and $f(t,y_1(t),\dots,y_n(t)) = (f_1,\dots,f_n)$ we get what is called the \textbf{vector form} of the system \ref{3.2.1}.

\begin{equation}\label{3.2.2}
  y'(t) = f (t, y(t))
\end{equation}

\begin{definition}
  For a function $y:D \subseteq \mathbb{R} \rightarrow \mathbb{R}^n$ , $|D| > 1$, if $y \in C^1(D,\mathbb{R}^n)$, and $y'(t) = f(t,y(t)), \forall t \in D \Rightarrow$ $y$ is a \textbf{solution of the system} \ref{3.2.2}.
\end{definition}

\begin{definition}
  An \textbf{autonomous equation} is a differential equation that does not explicitly depend on time, but is instead only defined by the relation between the function itself and its derivatives, so an equation of the form
  \begin{equation}\label{eq:3.2.3}
    f(y^{(n)}(t), y^{(n-1)}(t), \dots, y(t))= 0
  \end{equation}
  That is not to say, the function itself does not depend on time, but rather the independent variable $t$ does not explicitly appear in the equation.
\end{definition}

One more famous such example is the equation for the damped oscillator
\begin{equation}\label{eq:3.2.4}
  \ddot{x} + 2\gamma\dot{x} + \omega^2x = 0.
\end{equation}

Where:
\begin{itemize}
  \item $\gamma$ is the damping factor, meaning how quickly the oscillations of the system decay due to loss of momentum (caused by air resistance or friction)

  \item $\omega$ represents the angular (natural) frequency the oscillator would have, were it not to be affected by damping, defined by
    $\omega = \sqrt{\frac{k}{m}}$; where $k$ is the \textbf{elastic stiffness coefficient} of the spring and $m$ the mass.
\end{itemize}

In addition, a frequently used notation in physics is
$\dot{x} = \frac{d}{dt}x$ , $\ddot{x}=\frac{d^2}{dt^2}x$

That equation is a \textbf{linear} homogenous equation with constant coefficients (hence autonomous), which is pretty nice to work with. Their general form would be:
\begin{equation}\label{lin_hom_eq_const}
  x + a_1\dot{x}+ \dots +a_{n} x^{(n)} = 0
\end{equation}

Finding exact solutions, though, can be done by using some nice properties of Euler's exponential function $e^{rt}$.

Introducing the notion of \textbf{operators} would help in writing this next section.

\begin{definition}
  $D = \frac{d}{dt}$ is the \textbf{derivation operator}:
  $D^n = \frac{d^n}{dt^n}$, so $D^nu = D^n(u) =\frac{d^nu}{dt^n}$ where $u=u(t)$ and (related to \ref{lin_hom_eq_const}): \par
  \hspace{20pt} $L = a_1D + \dots + a_nD^n$ \par
  \hspace{20pt} is the \textbf{linear differential operator of order n}. Meaning \ref{lin_hom_eq_const} $\iff Lx=0$.
\end{definition}

We now can; just by pure Ansatz; assume the solutions take the form $e^{rt}$ for some (yet unknown) $r$'s. Because the exponential is so easy to work with when it comes to taking derivatives - a.k.a. : $D^n(e^{rt})=r^ne^{rt}$, so its derivatives are proportional to itself, meaning no new $t$'s are introduced - substituting back into \ref{lin_hom_eq_const} yields
\begin{gather*}
  Le^{rt} = e^{rt} + a_1 De^{rt} + \dots +a_{n} D^ne^{rt} = 0      \\
  \Updownarrow \\
  e^{rt}+a_1re^{rt} + \dots + a_nr^ne^{rt} =0 \\
  \Updownarrow \\
  1 + a_1r + \dots + a_nr^n =0
\end{gather*}
The left-hand-side is the \textbf{characteristic polynomial} of the linear equation.
\begin{equation}\label{lin_eq_char_poly}
  p(r) = a_1r + \dots + a_nr^n
\end{equation}
Notice that $L = p(D)$. \\
So now it all comes down to finding its roots $r$.

Case \rom{1}: A \textbf{unique, real} root found:

now $e^{rt}$ is a \textbf{fundamental solution} for $Le^{rt}=0$

Case \rom{2}: A \textbf{unique, complex} root found:

say it has the form $r = \lambda \pm i\mu, \mu \neq 0$. \\
By Euler's formula:
\begin{equation}\label{euler}
  e^{rt} = e^{(\lambda + i\mu)t} = e^{\lambda t} \cdot e^{i \mu t}  =  e^{\lambda t} \cdot [\text{cos}(\mu t) + i \text{sin}(\mu t)]
\end{equation}

So $e^{\lambda t}\text{cos}(\mu t)$ and $e^{\lambda t}\text{sin}(\mu t)$ are both fundamental solutions for $Lx=0$.

\textbf{Proof}: \par
Since $a_i \in \mathbb{R}, \forall i=\overline{1,n}$ in $p \implies p(\overline{r})=\overline{p(r)}, \forall r \in \mathbb{C}$, hence, just like 2AM coffee and the exam session, complex roots of $p$ always come in pairs. They both satisfy:
\begin{equation}\label{lin_op_solution}
  Le^{(\lambda \pm i \mu)} = 0
\end{equation}

But since our $x : \mathbb{R} \rightarrow \mathbb{R}$ we're looking for \textbf{real} valued solutions.
Because $\mu \in \mathbb{R}$ the following properties hold equivalently for $\lambda + i\mu$ and for $\overline{\lambda + i\mu}=\lambda - i\mu$ so I'll only write $\lambda \textbf{+} i \mu$ for simplicity. By \ref{euler}:
\begin{equation}\label{eulers_real_and_imaginary}
  \text{Re}(e^{rt})=e^{\lambda t} \cdot \text{cos}(\mu t), \hspace{2cm}    \text{Im}(e^{rt}) = e^{\lambda t} \cdot \text{sin}(\mu t).
\end{equation}
But (\textbf{reminder})
\begin{gather}\label{real_imag_another_way_to_write}
  \text{Re}(c)= \frac{c+\overline{c}}{2}, \hspace{2cm}
  \text{Im}(c) = \frac{c-\overline{c}}{2i}
\end{gather}
\begin{gather*}
  \forall c \in \mathbb{C}
\end{gather*}

\newcommand\firstConclusion{\stackrel{\mathclap{\normalfont\mbox{\ref{eulers_real_and_imaginary}, \ref{real_imag_another_way_to_write}}}}{=\joinrel=\joinrel=\joinrel=\joinrel=\joinrel=}}

\newcommand\byDistributivity{\stackrel{\mathclap{\normalfont\mbox{dist.}}}{=\joinrel=\joinrel=}}

\newcommand\operatorSatisfy{\stackrel{\mathclap{\normalfont\mbox{\ref{lin_op_solution}}}}{=\joinrel=}}

Using these three conclusions we get:
\begin{align*}
  L(e^{\lambda t} \cdot \text{cos}(\mu t)) \firstConclusion
  L\left(\frac{e^{rt}+e^{\overline{r}t}}{2} \right) \byDistributivity
  \frac{1}{2} \left( Le^{rt} + Le^{\overline{r}t} \right) \operatorSatisfy 0 , \\
  L(e^{\lambda t} \cdot \text{sin}(\mu t)) \firstConclusion
  L\left(\frac{e^{rt}-e^{\overline{r}t}}{2i} \right) \byDistributivity
  \frac{1}{2i} \left( Le^{rt} - Le^{\overline{r}t} \right) \operatorSatisfy 0 ,
\end{align*}

In conclusion, if $Le^{ct}=0$ for some $c \in \mathbb{C} \implies L\text{Re}(e^{ ct })= L\text{Im}(e^{ct})=0, \\
\text{for } \text{Re}(e^{ct}), \text{Im}(e^{ct}) \in \mathbb{R}$.
Moreover, the same can be said for $e^{\overline{c}t}$. \qed

Now something let's say... \textbf{fundamental} (hehe) about these solutions, is they have to be linearly independent to each other. Meaning the only solution for
\begin{equation}\label{liniar_independece}
  c_1 x_1+c_2 x_2 + \dots + c^n x^n = 0
\end{equation}
is the trivial solution $c_1=c_2=\dots=c_n=0$ where $x_i$ are the fundamental solutions of the system.

So the reason I've emphasized "\textbf{unique}" in the first 2 cases was because if we're to apply those methods for \textbf{repeating} roots, meaning the ones with multiplicity $k>1$, the resulting solutions wouldn't hold \ref{liniar_independece} because they would in fact be the same!

To counteract this issue, the clever people over at the 1748 Berlin Academy (just Euler) came up with yet another (easily provable) Ansatz: just multiplying by powers of $x$.

A root with multiplicity $k$ would introduce yet another $k$ -for real- or $2k$ - for complex - fundamental solutions to this equation, so to finish what we've started:

Case \rom{3}: \textbf{Repeated, real} roots $r$ with multiplicity $k>1$ yield the fundamental solutions
\[
  e^{rt}, xe^{rt}, \dots,x^{k-1}e^{rt}.
\]

These can be verified that they do, in fact satisfy \ref{liniar_independece}. So can:

Case \rom{4}: \textbf{Repeated, complex} roots $r$ with multiplicity $k>1$. These introduce the pairs:
\begin{align*}
  e^{at}\text{cos}(bt), \ \  e^{at}\text{sin}(bt)  \\
  xe^{at}\text{cos}(bt), \ \ xe^{at}\text{sin}(bt) \\
  \vdots                                           \\
  x^{k-1}e^{at}\text{cos}(bt), \ \ x^{k-1}e^{at}\text{sin}(bt)
\end{align*}

So $2k$ new solutions.

\textbf{The superposition principle}

Bernoulli (to the disbelief of Euler and Langrange) told us that the complex behaviour of vibrating strings can be modeled by separating their motion into well-defined and easy to compute simple waves with known frequencies and amplitudes, which can later be \textbf{superposed} with one-another to form the complete system.

This principle can be applied to \textit{our} easier to compute \textbf{fundamental solutions} ($x_1,\dots,x_n$) in relation to the system formed by our linear operator $Lx =0$.

Since it's a linear operator, additivity and homogeneity apply:
\begin{gather*}
  L(x_1+x_2) = Lx_1 + Lx_2\\
  aLx = L(ax) , \forall a \in \mathbb{C}
\end{gather*}
where $x,x_1,x_2$ are fundamental solutions of the system.

Hence the complete, final, \textbf{general} solution for \ref{lin_hom_eq_const} are linear combinations of all our previously found fundamental solutions:
\[
  x(t) = c_1 x_1 + \dots + c_nx_n
\]
for some constants $c_i$.
\newpage
\section{Formally defining and characterizing dynamical systems}
The reason why autonomous equations are so useful to us is because of the ease with which we can visualize the \textbf{evolution} of a system of such equations.
Take, for example:
\begin{definition}
  A \textbf{system of first-order autonomous equations} is a system of equations of the form
  \begin{equation}\label{fo_system_auton_eq}
    \begin{cases}
      \dot{y}_1 = f_1(y_1,\dots, y_n) \\
      \dot{y}_2 = f_2(y_1,\dots, y_n) \\
      \vdots                          \\
      \dot{y}_n = f_n(y_1,\dots, y_n) \\
    \end{cases}
  \end{equation}
\end{definition}

\begin{definition}
  We can construct the \textbf{phase space} of the \ref{fo_system_auton_eq} dynamical system by:

  \begin{enumerate}
    \item Representing all possible values of $y_i$, $ i = \overline{1,n}$ as points in Euclidean space $S \subseteq \mathbb{R}^n$

    \item Constructing a function $f : S \rightarrow S, f(y_1,y_2,\dots,y_n) = (\dot{y}_1,\dot{y}_2,\dots,\dot{y}_n)$ that attaches to each point in $S$ a vector representing the direction in which to go such that the relation \ref{fo_system_auton_eq} is satisfied.
  \end{enumerate}
\end{definition}

Thus, the \textbf{phase space} of the system is a vector field where $f$ is a continuous vector valued function.

So keeping this in mind, we may form a formal definition:

\begin{definition}
  A \textbf{dynamical system} is a tuple $(T,S,\Phi)$ where:

  \begin{itemize}
    \item $T$: part of the $(T,+)$ monoid of all possible $time$ moments of our system
    \item $S$: is the set of all possible states, and
    \item the function: $\Phi : U \subseteq (T \times S ) \rightarrow S$ where values of the second component span all of $S$, meaning $proj_2(U) = S$.
  \end{itemize}
  In order to give the necessary properties of $\Phi$ we will need to also define:
  $I(s):= \{ t \in T | (t,s) \in U \}, \forall s \in S$

  so $\Phi$ has the properties that:

  $\Phi(0,s) = s$

  $\Phi(t_2,\Phi(t_1,s)) = \Phi(t_1+t_2,s); \forall t_1, t_1+t_2 \in I(s), t_2 \in I(\Phi(t_1,s)), s \in S$.

  We call such a function $\Phi$ the \textbf{evolution function} of the dynamical system, mapping every state and $time$ after which that specific state is found, to another unique state.
\end{definition}

If we take a specific state $s$ as an initial constant, we can form the function

$\Phi_s : I(s) \rightarrow S$
called the \textbf{flow} through $s$, whose image:

$\gamma_s:=\{\Phi_s(t) | t\in I(s) \}$
is the \textbf{orbit} through $s$,
which is a curve of parameter $t \in I(s)$ representing the values the dynamical system will take throughout the experiment's timespan, given the initial condition (state) $s$.

Since a point only belongs to one orbit, if we form the set
$\Gamma_{U,\Phi}:=\{ \gamma_s | s \in S \}$
of all orbits of the dynamical system $(U,\Phi)$, with $U\subseteq (T \times S)$ and $\Phi$ the evolution function, it follows that:
$\forall \gamma_1,\gamma_2 \in \Gamma_{U, \Phi}, \gamma_1 \bigcap \gamma_2 = \emptyset$, meaning the set of orbits forms a partition of the phase space $S$

\textbf{Proof:}
Conditions for partitions:
\par \textbf{Reminder:} A partition $\Gamma_U$ of a set $U$ is a set of non-empty disjoint subsets of $U$ s.t.

\[
  \bigcup_{\gamma\in \Gamma_U} \gamma = U
\]

So we need to prove:

TODO: te apuci de asta mai incolo \par
\rom{1}. $\gamma \subseteq U, \forall \gamma \in \Gamma_{U,\Phi}$ \par
\rom{2}. $ \gamma \neq \emptyset, \forall \gamma \in \Gamma_{U,\Phi}$ \par
\rom{3}. $\gamma_1 \bigcap \gamma_2 = \emptyset, \forall \gamma_1, \gamma_2 \in \Gamma_{U,\Phi}$ \par
\rom{4}. $\bigcup_{\gamma\in \Gamma_U} \gamma = U$ \par

TODO: oki, deci aici zici cum se rezolva o ecuatie autonoma liniara in mod normal si despre solutiile fundamentale si independenta si dependa lor and stuff si dupa dai un mod echivalnt de a le rezolva cu metoda de mai jos care face un sistem din ele

We may, in fact construct an equivalent system
\[
  \begin{cases}
    \dot{y}_1 = f_1(y_1,\dots, y_n) \\
    \dot{y}_2 = f_2(y_1,\dots, y_n) \\
    \vdots                          \\
    \dot{y}_n = f_n(y_1,\dots, y_n) \\
  \end{cases}
\]

Starting from any autonomous differential equation
\[
  f(y^{(n)}(t), y^{(n-1)}(t), \dots, y(t))= 0
\]
via a technique which associates to each respective derivative a state unknown (variable).
This system's form depends on the solvability with respect to the highest derivative $y^{(n)}$.
The first form may be obtained if the above equation \textbf{can} be solved in terms of $y^{(n)}$, meaning it can be re-written as
\begin{gather*}
  f(y^{(n)}(t), y^{(n-1)}(t), \dots, y(t))= 0 \\
  \Updownarrow \\
  y^{(n)}  = G(y^{(n-1)}(t), \dots, y(t))
\end{gather*}

We first take each state unknown to be equal to their respective derivatives, so:
\begin{gather*}
  y_1 = y     \\
  y_2 = \dot{y} \\
  \vdots \\
  y_n = y^{(n-1)}
\end{gather*}

Then we take the derivatives with respect to time of each of these equations, so starting with $\dot{y}_1 = \dot{y}$, but $\dot{y} = y_2$, so $\dot{y_1}=y_2$. Continuing on like that, in a sort of diagonal fashion for all but the last state variable, we get:
\begin{gather*}
  \dot{y}_1 = y_2 \\
  \dot{y}_2 = y_3 \\
  \vdots \\
  \dot{y}_{n-1} = y_n
\end{gather*}
and this is where that beginning "solvability" part comes into play. The very last one will be exactly its solution defined earlier:
\[
  \dot{y}_n = \frac{d}{dt}y^{(n-1)} = y^{(n)}  = G(y^{(n-1)}(t), \dots, y(t))
\]

So, putting this all together we can get the equivalent system of the autonomous differential equation
\[
  \begin{cases}
    \dot{y}_1 = y_2     \\
    \dot{y}_2 = y_3     \\
    \vdots              \\
    \dot{y}_{n-1} = y_n \\
    \dot{y}_n  = G(y^{(n-1)}(t), \dots, y(t))
  \end{cases}
\]

Now, if instead of an actual function $G$ we only get an implicit form for a solution (\ref{implicit_func_theorem} isn't satisfied), we could write the original equation in terms of these new state variables:
\begin{gather*}
  f(y(t), \dot{y}(t),\dots,y^{(n-1)},y^{(n)})= 0 \\
  \Updownarrow \\
  f(y_1,\dots,y_n,\dot{y}_n) = 0
\end{gather*}

So the system would be:
\[
  \begin{cases}
    \dot{y}_1 = y_2     \\
    \dot{y}_2 = y_3     \\
    \vdots              \\
    \dot{y}_{n-1} = y_n \\
    f(y_1,\dots,y_n,\dot{y}_n) = 0
  \end{cases}
\]

This is a differential-algebraic system of equations (DAE), which is beyond the scope of this thesis but figured I'd cover all cases (this one's usually solved via numerical methods).

Okay, but now since we know how to form systems from any autonomous equation \dots how does that make our lives easier in any way?

Let's consider \ref{lin_hom_eq_const}, the equation we worked with previously. Now we can express it differently:

We could first divide everything by $a_{n}$ so everything looks a bit better.
\begin{gather*}
  \left. a_0 x + a_1\dot{x}+ \dots +a_{n} x^{(n)} = 0 \middle| \frac{1}{a_{n}} \right. \\
  \Updownarrow \\
  b_0 x + b_1\dot{x} + \dots + b_{n-1} x^{(n-1)} + x^{(n)} = 0
\end{gather*}

And now apply the aforementioned technique:
\begin{gather*}
  x_1 = x     \\
  x_2 = \dot{x} \\
  \vdots \\
  x_n = x^{(n-1)}
\end{gather*}

then:
\[
  \begin{cases}
    \dot{x}_1 = x_2     \\
    \dot{x}_2 = x_3     \\
    \vdots              \\
    \dot{x}_{n-1} = x_n \\
    \dot{x}_n  = x^{(n)} = -b_0x - b_1\dot{x} - \dots - b_{n-1} x^{(n-1)} = -b_0 x_1 - b_1 x_2 - \dots - b_n x_{n-1}
  \end{cases}
\]

Waitt, I saw this somewhere in year 1, semester 1 during Linear Algebra (which I definitely didn't have to re-take twice)

That is the same as:
\begin{align*}
  \begin{bmatrix}
    \dot{x}_1     \\
    \dot{x}_2     \\
    \dot{x}_3     \\
    \vdots        \\
    \dot{x}_{n-1} \\
    \dot{x}_n
  \end{bmatrix} =
  \bordermatrix{ & x_1    & x_2    & x_3    & \ldots & x_n \cr
    & 0      & 1      & 0      & \ldots & 0 \cr
    & 0      & 0      & 1      & \ldots & 0 \cr
    & 0      & 0      & 0      & \ldots & 0 \cr
    & \vdots & \vdots & \vdots & \ddots & \vdots \cr
    & 0      & 0      & 0      & \ldots & 1 \cr
  & -b_0   & -b_1   & -b_2   & \ldots & b_n }
  \begin{bmatrix}
    x_1     \\
    x_2     \\
    x_3     \\
    \vdots  \\
    x_{n-1} \\
    x_n
  \end{bmatrix}
\end{align*}

Writing it in compact form:
\[
  \dot{x} = A x
\]
With $x, \dot{x}$ 2 $n$ sized column vectors and $A$ an $n \bigtimes n$ matrix.

Now notice that
\[
  \text{det}(A- r I_n)
\]

Is precisely the char. poly. \ref{lin_eq_char_poly}. So the $r$ solutions for
\[
  \text{det}(A - r I_n) =0
\]
(a.k.a the eigenvalues of the A matrix) are also the roots of \ref{lin_eq_char_poly}. So solving for them would obtain the solutions
\[
  x_i = e^{rt}x_i(0)
\]
for $i = \overline{1,n}$.

A much simpler example of a system that can be solved by not even having to make such computations is that of a \textbf{decoupled} dyamical system. Alas one where each state variable only depends on itself. Common examples that come to mind would be the concentration of a homogenous substance in an isolated tank while under the influence of heat transfer. This wouldn't really be a "system" so we could take multiple such tanks in a lab, where they are indeed sepparated from one-another but still form the system.
\begin{equation*}
  \begin{cases}
    \dot{x}_1 = \lambda_1 x_1 \\
    \dot{x}_2 = \lambda_2 x_2 \\
    \vdots \\
    \dot{x}_n = \lambda_n x_n
  \end{cases}
\end{equation*}

The system's matrix $D$ is now:
\begin{equation*}
  D =
  \begin{bmatrix}
    \lambda_1 & 0 & 0 &\dots & 0 \\
    0 & \lambda_2 & 0 & \dots & 0 \\
    0 & 0 & \lambda_3 & \dots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots & \lambda_n
  \end{bmatrix}
\end{equation*}

We notice that our solutions will now take the form
\[
  x = e^{Dt}x(0)
\]

where $\cdot^A : \mathbb{R} \rightarrow M_n(\mathbb{R})$ for an $A = (a)_{ij} \in M_n(\mathbb{R})$ is defined as:
\[
  \alpha^A = (\alpha^a)_{ij}
\]

so
\begin{align*}
  \underline{x}(t) =
  \begin{bmatrix}
    e^{\lambda_1} & 0 & 0 &\dots & 0 \\
    0 & e^{\lambda_2} & 0 & \dots & 0 \\
    0 & 0 & e^{\lambda_3} & \dots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots & e^{\lambda_n}
  \end{bmatrix}
  \underline{x}(0)
\end{align*}
Where I use $\underline{x}$ to denote the \textbf{vector form} of a the state-space vars.
In conclusion, it'd be ideal for us to work with such \textbf{diagonal} matrices.

There are, however situations when we don't have that luxury, so we could instead \textbf{transofrm} the way we write our $x$'s (and hence, our transformation as well) in order for us \textbf{diagonalize} our system. This would decouple the system and simplify the computations.

This is pretty straight-forward:

Starting from the system
\begin{equation}\label{diag_matrix_init_sys}
  \dot{x} = A x
\end{equation}
we need to find a transformation $T$ from our coordinate system
\begin{equation}\label{diag_trans_rel}
  x = Tz
\end{equation}
s.t. our dynamics are decoupled (the new transformation matrix $D$ is diagonal)
\[
  \dot{z} = Dz
\]
Derivating \ref{diag_trans_rel} and substituting into \ref{diag_matrix_init_sys}:
\begin{gather*}
  \left. x = Tz \middle| \frac{d}{dt} \right. \iff
  \dot{x} = T\dot{z} = Ax = ATz \Rightarrow \dot{z} = T^{-1} A T z = Dz \\
  \text{Which allows us to find an invertible } T \text{ s.t.} \\
  \text{the diagonal matrix } D \text{ is the conjugate of (similar to) } A \text{ w.r.t. } T \\
  \left. T^{-1} A T z = Dz \middle| \frac{T}{z}
  \right. \\
  \Updownarrow \\
  A T = T D.
\end{gather*}

Everything comes down to solving the above eigenvalue problem.

As stated in the above sections: $diag(D) = \{ \lambda | det(A - \lambda I_n) = 0 \}$ and it can also be found that the columns of $T$ are $A$'s corresponding eigenvectors $\xi_i$.

After computing $A$'s eigenvalues, we've essentially also found $T$, now
\[
  D = T^{-1} A T
\]
is used to find the diagonal matrix which will decouple our dynamics in the new coordinate system $z = T^{-1} x$; $\dot{z} = D z$.

Transforming back into our world, the solutions (in vector form) are:
\begin{gather*}
  \underline{z}(t) = e^{Dt}\underline{z}(0) \\
  \Updownarrow \\
  T^{-1} \underline{x}(t) = e^{Dt} T^{-1} \underline{x}(0) \\
  \Updownarrow \\
  \underline{x}(t) = T e^{Dt} T^{-1} \underline{x}(0). \\
  \Updownarrow \\
  \underline{x}(t) = T e^{Dt} \underline{z}(0). \\
  \Updownarrow \\
  \underline{x}(t) = T \underline{z}(t).
\end{gather*}
Now we know the equations have the form
\[
  x_i = T z_i(t), i = \overline{1,n}.
\]
Where $T$ is the $n \bigtimes n$ matrix whose columns are $A$ (not necesarily linearly independent) eigenvectors.
\begin{equation*}
  \begin{bmatrix*}
    \vdots & \vdots & \dots & \vdots \\
    \xi_1 & \xi_2 & \dots & \xi_n \\
    \vdots & \vdots & \dots & \vdots \\
  \end{bmatrix*}
\end{equation*}
Boom! You're golden.

Now the transformation only scales the basis vectors in our new coordinate system and we can use this to our advantage to find solutions in that system, then turn them back into our original system to see which values they correspond to.
\vskip\bigskipamount

Okay but what if my system is not linear? \textbf{Then} how will forming its corresponding system help me?

\vskip\bigskipamount
Si dai exemplul cu pendumulul, reintroducand idea de phase space si cum poti sa-l formezi tu acum folosind metoda de mai sus (o sa-i dau un nume mai incolo s-o referentiezi)
\vskip\bigskipamount

Taking, in particular, a more complex autonomous differential equation; the one for the damped pendulum:

\begin{equation}\label{damped_pendulum}
  \ddot{\theta} +\mu\dot{\theta} + \frac{g}{L}\sin(\theta) = 0
\end{equation}
where:  \par
$
\left.
\begin{array}{l}
  \theta : \text{angular position}       \\
  \dot{\theta} : \text{angular velocity} \\
  \ddot{\theta} : \text{angular acceleration}
\end{array}
\right\}
\text{all functions of } t \text{ (time)}
$

\ \ $\mu = \frac{c}{mL}$ : dampening factor \par
\ \ $c$ : the dampening coefficient due to the resistive forces \par
\ \ $m$: mass at the end of the pendulum \par
\ \ $g \approx 9.81 \frac{m}{s^2}$: earth's gravitational acceleration \par
\ \ $L$: the pendulum's length \par

We can express $\theta$ and $\dot{\theta}$ as two separate and independent variables in a 2D space of points \par $(\theta,\dot{\theta}) = (\theta,\omega)$.
And attach to each point the vector formed from the derivatives of each component, which would be (according to \ref{damped_pendulum}):

\begin{align}
\frac{d}{dt}
\begin{bmatrix}
  \theta \\
  \dot{\theta}
\end{bmatrix} =
\begin{bmatrix}
  \dot{\theta} \\
  \ddot{\theta}
\end{bmatrix} =
\begin{bmatrix}
  \omega \\
  -\mu \cdot \omega - \frac{g}{L}\sin(\theta)
\end{bmatrix}
\end{align}

This would in turn form the phase portrait of the dynamical system, described by a system of first-order autonomous differential equations:

\[
\begin{cases}
  \dot{\theta}  = \omega \\
  \dot{\omega} = -\mu \cdot \omega - \frac{g}{L}\sin(\theta)
\end{cases}
\]

\vskip\bigskipamount
TODO: faci tu dupa aici un plot in matlab frumusel
si dupa mai gasesti un exemplu mistio in 3d

arat dupa ce faci toate demonstratiile pentru n dimensiuni, niste poze si cazuri particulare in 2,3 dimensiuni, cv ce se paote vedea cu poze

arata in state spaceu ala cum iti dai seama stabilitatea sistemului dand graficul oribtelor in vector spaceu ala ( sa mai pui si snite poze)
\vskip\bigskipamount

We can observe that the pendulum will not move an inch if it is just left in its "equilibrium" point hanging down ($\omega \equiv \theta \equiv 0$), but that like a coin left on its side, it will also stay still if started at the \textbf{polar} opposite angle in relation to the stability angle ($\omega =0 , \theta = 180 \degree)$, standing up.
Although, like a coin left standing on its side, even the slightest deviation will get it tumbling down. This can be observed in the system's phase portrait.

\newpage

The study of how much such changes in initial conditions (or just the passing of time itself) influence later behaviour is what we call the \textbf{stability}.

In general, for a system: \ref{fo_system_auton_eq}, written in the compact form
\[
\dot{y}(t) = \underline{f}(y(t)).
\]

we have:
\begin{definition}
\textbf{Equilibrium (fixed) points} $y_0$ those satisfying:

$ \underline{f}(y_0) = 0 = \dot{y}(t) $

Which are classified into:

\textbf{Stable equilibriums}:
A point $y_0 \in S$ that:

$\qquad \qquad \forall \epsilon > 0, \exists \delta > 0 : $:
\begin{equation} \label{cond:stable1}
  \text{\rom{1}: } \forall y(0) \in S \text{ for which } \norm{y(0) - y_0} < \delta \implies \exists y(t), \forall t >= 0.  \tag{\rom{1}}
\end{equation}
\begin{equation} \label{cond:stable2}
  \text{\rom{2}: } \norm{ y(t) - y_0 } < \epsilon \quad \forall t >= 0. \tag{\rom{2}}
\end{equation}

\textbf{Unstable equilibriums (or repeller)}:
If either \eqref{cond:stable1} or \eqref{cond:stable2} don't hold

\textbf{Asymptotically stable equilibriums (or attractors)}:
Subset of \textbf{stable equilibria} for which:
\begin{equation}\label{cond:asympt_stable}
  \text{\rom{3}: } \lim_{t \rightarrow \infty} \norm{y(t) - y_0}  = 0 \text{ for those whose } \norm{ y(0) - y_0} < \delta \tag{\rom{3}}
\end{equation}

And as opposed to, say  a continuum of fixed points we have:

\textbf{Isolated equilibrium}:

A fixed point $y_0$ is said to be \textbf{isolated} if
\[
  \exists \delta > 0 : \forall y \text{ satifying } \norm{y - y_0 } < \delta, \underline{f}(y) \neq 0.
\]
\end{definition}

Classifying the stabilty for a system comes down to looking at the way each state variable evolves with time.

For a linear system, looking at the eigenvalues of the transformation is enough to classify the stabilty of the entire system; taking into consideration how the solutions look; like so:

Taking the eigenvalues
\begin{gather*}
\lambda_1 \dots \lambda_n; \\
\text{ \rom{1} if: } Re(\lambda_i) < 0, \forall i = \overline{1,n} \implies \text{the system is stable }. \\
\text{ \rom{2} if: } \exists I \subset \mathbb{N} \text{ a set of indices } : \forall i \in I, Re(\lambda_i) = 0 \text{ and } \\
\forall k \in \{ 1 , \dots n \} \setminus I, Re(\lambda_k) < 0 \implies  \text{ the system is only \textbf{marginally stable.}} \\
\text{ \rom{3} if: } \exists i = \overline{1,n} : Re(\lambda_i) > 0 \implies \text{system is \textbf{unstable}.}
\end{gather*}

TODO:
5. arata poza de clasificarea sistemelor, dat
\vskip\bigskipamount